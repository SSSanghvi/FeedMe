{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn \n",
    "import pandas as pd\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/processed_dishes_v3.csv')\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to string\n",
    "data['menu_section'] = data['menu_section'].values.astype('str')\n",
    "data['dish_name'] = data['dish_name'].values.astype('str')\n",
    "data['cleaned_descriptions'] = data['cleaned_descriptions'].values.astype('str')\n",
    "data['full_description'] = data['full_description'].values.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['full_description']\n",
    "Y = data.loc[:, 'contains_peanuts':'contains_meat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Representation\n",
    "\n",
    "Most classifiers and learning algorithms require the input data to be in numerical format rather than strings. Therefore, using a measure called Term Frequency, Inverse Document Frequency (tf-idf), I will convert the strings into vectors of integers. I have chosen a `min_df` value of 5, which means that a word must be present at least 5 times to be kept. This will help us remove any necessary words, especially since we've included the dish name as part of the features, and some names may be more fun than informative. I have also chosen the `ngram_range` to be `(1, 2)`, indicating that we want unigrams and bigrams. This is because certain food phrases may be more than 1 word long, and capturing those phrases is equally as important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df=5, ngram_range=(1, 2))\n",
    "tfidf_features = tfidf.fit_transform(X).toarray()\n",
    "tfidf_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using OneVsRest Classifier for Multi-Label Classification\n",
    "Because our problem involves multi-label classification, one suggestion was to use the OneVsRest Classifier module. According to [this source](https://towardsdatascience.com/journey-to-the-center-of-multi-label-classification-384c40229bff):\n",
    "> In an “one-to-rest” strategy, one could build multiple independent classifiers and, for an unseen instance, choose the class for which the confidence is maximized. The main assumption here is that the labels are mutually exclusive. You do not consider any underlying correlation between the classes in this method.\n",
    "\n",
    "We need to use this because models like Logistic Regression and Naive Bayes only take in a 1-D array of labels. However, we have 7 different labels since we're essentially doing multi-label classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(tfidf_features, Y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['contains_peanuts', 'contains_egg', 'contains_sesame',\n",
    "              'contains_fish', 'contains_shellfish', 'contains_soy',\n",
    "              'contains_meat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Using pipeline for applying logistic regression and one vs rest classifier\n",
    "LogReg_pipeline = Pipeline([\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression())),\n",
    "            ])\n",
    "for category in categories:\n",
    "    print('**Processing {}...**'.format(category))\n",
    "    \n",
    "    # Training logistic regression model on train data\n",
    "    LogReg_pipeline.fit(x_train, y_train[category])\n",
    "    \n",
    "    # calculating test accuracy\n",
    "    prediction = LogReg_pipeline.predict(x_test)\n",
    "    print('Test accuracy is {}'.format(accuracy_score(y_test[category], prediction)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our Naive Bayes model, I am using MultinomialNB because as stated in [this source](https://scikit-learn.org/stable/modules/naive_bayes.html): \n",
    "> MultinomialNB implements the naive Bayes algorithm for multinomially distributed data, and is one of the two classic naive Bayes variants used in text classification (where the data are typically represented as word vector counts, although tf-idf vectors are also known to work well in practice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating pipeline for multinomial nb model\n",
    "model_nb = MultinomialNB()\n",
    "NB_pipeline = Pipeline([\n",
    "                ('clf', OneVsRestClassifier(model_nb)),\n",
    "            ])\n",
    "for category in categories:\n",
    "    print('**Processing {}...**'.format(category))\n",
    "\n",
    "    # Training naive bayes model on train data\n",
    "    NB_pipeline.fit(x_train, y_train[category])\n",
    "    \n",
    "    # calculating test accuracy\n",
    "    prediction = NB_pipeline.predict(x_test)\n",
    "    print('Test accuracy is {}'.format(accuracy_score(y_test[category], prediction)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating pipeline for svm model\n",
    "model_svm = LinearSVC()\n",
    "SVM_pipeline = Pipeline([\n",
    "                ('clf', OneVsRestClassifier(model_svm)),\n",
    "            ])\n",
    "for category in categories:\n",
    "    print('**Processing {}...**'.format(category))\n",
    "\n",
    "    # Training SVM model on train data\n",
    "    SVM_pipeline.fit(x_train, y_train[category])\n",
    "    \n",
    "    # calculating test accuracy\n",
    "    prediction = SVM_pipeline.predict(x_test)\n",
    "    print('Test accuracy is {}'.format(accuracy_score(y_test[category], prediction)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Binary Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using binary relevance\n",
    "from skmultilearn.problem_transform import BinaryRelevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "classifier_lr = BinaryRelevance(LogisticRegression(random_state=42))\n",
    "classifier_lr.fit(x_train, y_train)\n",
    "predictions_br_lr = classifier_lr.predict(x_test)\n",
    "print(\"Accuracy = \", accuracy_score(y_test, predictions_br_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive bayes\n",
    "classifier_nb = BinaryRelevance(MultinomialNB())\n",
    "classifier_nb.fit(x_train, y_train)\n",
    "predictions_br_nb = classifier_nb.predict(x_test)\n",
    "print(\"Accuracy = \", accuracy_score(y_test, predictions_br_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm\n",
    "classifier_svm = BinaryRelevance(LinearSVC(random_state=42))\n",
    "classifier_svm.fit(x_train, y_train)\n",
    "predictions_br_svm = classifier_svm.predict(x_test)\n",
    "print(\"Accuracy = \", accuracy_score(y_test, predictions_br_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensembling\n",
    "final_pred = []\n",
    "for i in range(len(x_test)):\n",
    "    pred1 = predictions_br_lr.toarray()[i]\n",
    "    pred2 = predictions_br_nb.toarray()[i]\n",
    "    pred3 = predictions_br_svm.toarray()[i]\n",
    "    temp_pred = []\n",
    "    for j in range(7):\n",
    "        pred1_j = pred1[j]\n",
    "        pred2_j = pred2[j]\n",
    "        pred3_j = pred3[j]\n",
    "        temp_pred.append(int(np.round(np.mean([pred1_j, pred2_j, pred3_j*2]))))\n",
    "    final_pred.append(temp_pred)\n",
    "print(\"Accuracy = \", accuracy_score(y_test, final_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using KNeighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neighbor_range = range(2, 31)\n",
    "accuraries_neigh = []\n",
    "for i in neighbor_range:\n",
    "    neigh = KNeighborsClassifier(n_neighbors=i)\n",
    "    neigh.fit(x_train, y_train)\n",
    "    predictions_neigh = neigh.predict(x_test)\n",
    "    print(\"Accuracy for\", i, \"=\", accuracy_score(y_test, predictions_neigh))\n",
    "    accuraries_neigh.append(accuracy_score(y_test, predictions_neigh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
