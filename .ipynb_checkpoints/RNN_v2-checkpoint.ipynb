{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn \n",
    "import pandas as pd\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dish_id</th>\n",
       "      <th>menu_section</th>\n",
       "      <th>dish_name</th>\n",
       "      <th>menu_description</th>\n",
       "      <th>cleaned_descriptions</th>\n",
       "      <th>contains_peanuts</th>\n",
       "      <th>contains_egg</th>\n",
       "      <th>contains_sesame</th>\n",
       "      <th>contains_fish</th>\n",
       "      <th>contains_shellfish</th>\n",
       "      <th>contains_soy</th>\n",
       "      <th>contains_meat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>8c310c8c-2461-4360-b10e-d21f331d8a4f</td>\n",
       "      <td>Gluten Free Specialty Pies</td>\n",
       "      <td>Chicken Bacon Ranch</td>\n",
       "      <td>chicken, bacon, ranch sauce, mozzarella and ch...</td>\n",
       "      <td>chicken bacon ranch sauce mozzarella cheddar</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2aba477b-461d-48b9-86ab-4a488bc39940</td>\n",
       "      <td>Wraps &amp; Roll-Ups</td>\n",
       "      <td>Chinatown Chicken Wrap</td>\n",
       "      <td>tender chicken pieces simmered in oriental hon...</td>\n",
       "      <td>tender chicken pieces simmered oriental honey ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>54678a69-b974-4391-8933-911a851351aa</td>\n",
       "      <td>Pratos Tradicionais (Traditional Dishes)</td>\n",
       "      <td>Katchupa Ref (( small ))</td>\n",
       "      <td>refried katchupa with eggs and linguica.</td>\n",
       "      <td>refried katchupa eggs linguica</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6cb8a4f5-5ed5-4ebd-a8a9-5870a9127cf5</td>\n",
       "      <td>Club Sandwiches</td>\n",
       "      <td>Ham &amp; Cheese Club</td>\n",
       "      <td>served with mayo, lettuce, tomato, pickles and...</td>\n",
       "      <td>served mayo lettuce tomato pickles bacon</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>881bf589-645a-426b-a43e-09002cdbbb3e</td>\n",
       "      <td>Noodles (Kitchen Entrée)</td>\n",
       "      <td>Yaki Udon or Soba</td>\n",
       "      <td>pan fried udon noodles or wheat flour noodles ...</td>\n",
       "      <td>pan fried udon noodles wheat flour noodles cho...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                dish_id  \\\n",
       "0  8c310c8c-2461-4360-b10e-d21f331d8a4f   \n",
       "1  2aba477b-461d-48b9-86ab-4a488bc39940   \n",
       "2  54678a69-b974-4391-8933-911a851351aa   \n",
       "3  6cb8a4f5-5ed5-4ebd-a8a9-5870a9127cf5   \n",
       "4  881bf589-645a-426b-a43e-09002cdbbb3e   \n",
       "\n",
       "                               menu_section                 dish_name  \\\n",
       "0                Gluten Free Specialty Pies       Chicken Bacon Ranch   \n",
       "1                          Wraps & Roll-Ups    Chinatown Chicken Wrap   \n",
       "2  Pratos Tradicionais (Traditional Dishes)  Katchupa Ref (( small ))   \n",
       "3                           Club Sandwiches         Ham & Cheese Club   \n",
       "4                  Noodles (Kitchen Entrée)         Yaki Udon or Soba   \n",
       "\n",
       "                                    menu_description  \\\n",
       "0  chicken, bacon, ranch sauce, mozzarella and ch...   \n",
       "1  tender chicken pieces simmered in oriental hon...   \n",
       "2           refried katchupa with eggs and linguica.   \n",
       "3  served with mayo, lettuce, tomato, pickles and...   \n",
       "4  pan fried udon noodles or wheat flour noodles ...   \n",
       "\n",
       "                                cleaned_descriptions  contains_peanuts  \\\n",
       "0       chicken bacon ranch sauce mozzarella cheddar                 0   \n",
       "1  tender chicken pieces simmered oriental honey ...                 0   \n",
       "2                     refried katchupa eggs linguica                 0   \n",
       "3           served mayo lettuce tomato pickles bacon                 0   \n",
       "4  pan fried udon noodles wheat flour noodles cho...                 0   \n",
       "\n",
       "   contains_egg  contains_sesame  contains_fish  contains_shellfish  \\\n",
       "0             0                0              0                   0   \n",
       "1             0                0              0                   0   \n",
       "2             1                0              0                   0   \n",
       "3             0                0              0                   0   \n",
       "4             0                1              0                   0   \n",
       "\n",
       "   contains_soy  contains_meat  \n",
       "0             0              1  \n",
       "1             1              1  \n",
       "2             0              1  \n",
       "3             0              1  \n",
       "4             1              1  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/processed_dishes_v2.csv')\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to string\n",
    "data['menu_section'] = data['menu_section'].values.astype('str')\n",
    "data['dish_name'] = data['dish_name'].values.astype('str')\n",
    "data['cleaned_descriptions'] = data['cleaned_descriptions'].values.astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['cleaned_descriptions'] # description only\n",
    "Y = data['contains_peanuts'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 47694)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=1, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
    "features = tfidf.fit_transform(description_only).toarray()\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## end ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data.loc[:, 'dish_id':'cleaned_descriptions']\n",
    "Y = data.loc[:, 'contains_peanuts':'contains_meat'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_descriptions = features['cleaned_descriptions'].astype(str).apply(lambda s: s.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary: 3587\n"
     ]
    }
   ],
   "source": [
    "s = set()\n",
    "for d in split_descriptions:\n",
    "    [s.add(w) for w in d]\n",
    "print(f'Size of vocabulary: {len(s)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "description_only = features['cleaned_descriptions'].to_numpy()\n",
    "tokenizer = Tokenizer(num_words=5252)\n",
    "tokenizer.fit_on_texts(description_only)\n",
    "encoded_description = tokenizer.texts_to_matrix(description_only, mode='count')\n",
    "encoded_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Embedding(5252, 128))\n",
    "model.add(layers.LSTM(128))\n",
    "model.add(layers.Dense(7, activation='sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = tf.keras.preprocessing.sequence.pad_sequences(split_descriptions, dtype=object, padding='post', value=' ')\n",
    "# X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 2156s 9s/step - loss: 2.0134 - accuracy: 0.3360\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f84eaa38550>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(encoded_description, Y, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: assets\n"
     ]
    }
   ],
   "source": [
    "model.save('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.05101454, 0.28925067, 0.13817862, 0.0783166 , 0.11783832,\n",
       "        0.25789842, 0.9949615 ]], dtype=float32)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = ['chicken bacon ranch sauce mozzarella cheddar'] \n",
    "encoded_test = tokenizer.texts_to_matrix(test, mode='count')\n",
    "model.predict(encoded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
